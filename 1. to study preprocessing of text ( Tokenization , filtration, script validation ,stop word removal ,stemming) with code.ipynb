{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72d4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Text preprocessing is an important step in natural language processing. It involves tokenization, filtration, script validation, stop word removal, and stemming.\n",
      "\n",
      "Tokenization:\n",
      "['Text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'involves', 'tokenization', ',', 'filtration', ',', 'script', 'validation', ',', 'stop', 'word', 'removal', ',', 'and', 'stemming', '.']\n",
      "\n",
      "Filtration:\n",
      "['text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '', 'it', 'involves', 'tokenization', '', 'filtration', '', 'script', 'validation', '', 'stop', 'word', 'removal', '', 'and', 'stemming', '']\n",
      "\n",
      "Script Validation:\n",
      "['text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', 'it', 'involves', 'tokenization', 'filtration', 'script', 'validation', 'stop', 'word', 'removal', 'and', 'stemming']\n",
      "\n",
      "Stop Word Removal:\n",
      "['text', 'preprocessing', 'important', 'step', 'natural', 'language', 'processing', 'involves', 'tokenization', 'filtration', 'script', 'validation', 'stop', 'word', 'removal', 'stemming']\n",
      "\n",
      "Stemming:\n",
      "['text', 'preprocess', 'import', 'step', 'natur', 'languag', 'process', 'involv', 'token', 'filtrat', 'script', 'valid', 'stop', 'word', 'remov', 'stem']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample text\n",
    "text = \"Text preprocessing is an important step in natural language processing. It involves tokenization, filtration, script validation, stop word removal, and stemming.\"\n",
    "\n",
    "# Tokenization: Split the text into words or tokens\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Filtration: Remove non-alphanumeric characters and convert to lowercase\n",
    "filtered_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token).lower() for token in tokens]\n",
    "\n",
    "'''\n",
    "re.sub() function is used to substitute (replace) all characters in token that are not letters (a to z and A to Z) or digits (0 to 9) with an empty string ''. \n",
    "\n",
    "'''\n",
    "\n",
    "# Script Validation: You can use regular expressions to validate scripts (e.g., only keep words with Latin characters - letters from english alphabets, both uppercase and lowercase)\n",
    "latin_tokens = [token for token in filtered_tokens if re.match('^[a-zA-Z]+$', token)]\n",
    "\n",
    "# Stop Word Removal: Remove common stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens_no_stop = [token for token in latin_tokens if token not in stop_words]\n",
    "\n",
    "# Stemming: Reduce words to their root form using Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens_no_stop]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)\n",
    "print(\"\\nFiltration:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nScript Validation:\")\n",
    "print(latin_tokens)\n",
    "print(\"\\nStop Word Removal:\")\n",
    "print(filtered_tokens_no_stop)\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f99c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is assumed to be a list of tokens that have been previously processed and cleaned, likely by removing non-alphanumeric characters and converting the tokens to lowercase.\n",
      "\n",
      "Tokens - \n",
      "['This', 'is', 'assumed', 'to', 'be', 'a', 'list', 'of', 'tokens', 'that', 'have', 'been', 'previously', 'processed', 'and', 'cleaned', ',', 'likely', 'by', 'removing', 'non-alphanumeric', 'characters', 'and', 'converting', 'the', 'tokens', 'to', 'lowercase', '.'] \n",
      "\n",
      "Filtered tokens - \n",
      "['this', 'is', 'assumed', 'to', 'be', 'a', 'list', 'of', 'tokens', 'that', 'have', 'been', 'previously', 'processed', 'and', 'cleaned', '', 'likely', 'by', 'removing', 'nonalphanumeric', 'characters', 'and', 'converting', 'the', 'tokens', 'to', 'lowercase', '']\n",
      "\n",
      "Script Validation - \n",
      "['this', 'is', 'assumed', 'to', 'be', 'a', 'list', 'of', 'tokens', 'that', 'have', 'been', 'previously', 'processed', 'and', 'cleaned', 'likely', 'by', 'removing', 'nonalphanumeric', 'characters', 'and', 'converting', 'the', 'tokens', 'to', 'lowercase']\n",
      "\n",
      "Stop words removal - \n",
      "['assumed', 'list', 'tokens', 'previously', 'processed', 'cleaned', '', 'likely', 'removing', 'nonalphanumeric', 'characters', 'converting', 'tokens', 'lowercase', '']\n",
      "\n",
      "Stemmed tokens - \n",
      "['thi', 'is', 'assum', 'to', 'be', 'a', 'list', 'of', 'token', 'that', 'have', 'been', 'previous', 'process', 'and', 'clean', '', 'like', 'by', 'remov', 'nonalphanumer', 'charact', 'and', 'convert', 'the', 'token', 'to', 'lowercas', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# text = ' Veritatis obcaecati tenetur iure eius earum ut molestias architecto voluptate aliquam nihil, eveniet aliquid culpa officia aut! Impedit sit sunt quaerat, odit, tenetur error, harum nesciunt ipsum debitis quas aliquid.'\n",
    "\n",
    "text = 'This is assumed to be a list of tokens that have been previously processed and cleaned, likely by removing non-alphanumeric characters and converting the tokens to lowercase.'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token).lower() for token in tokens]\n",
    "\n",
    "latin_characters = [token for token in filtered_tokens if re.match('^[a-zA-Z]+$', token) ]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "no_stop_words = [token for token in filtered_tokens if token not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(f'Text: {text}\\n')\n",
    "print(f'Tokens - \\n{tokens} \\n')\n",
    "print(f'Filtered tokens - \\n{filtered_tokens}\\n')\n",
    "print(f'Script Validation - \\n{latin_characters}\\n')\n",
    "print(f'Stop words removal - \\n{no_stop_words}\\n')\n",
    "print(f'Stemmed tokens - \\n{stemmed_tokens}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1b9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
