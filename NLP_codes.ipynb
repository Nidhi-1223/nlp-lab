{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nidhi-1223/nlp-lab/blob/main/NLP_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 1 - Study preprocessing of text\n",
        "\n",
        "\n",
        "Theory:\n",
        "To preprocess your text simply means to bring your text into a form that is predictable and analyzable for your task. A task here is a combination of approach and domain.\n",
        "Machine Learning needs data in the numeric form. We basically used encoding techniques (BagOfWord, Bi-gram,n-gram, TF-IDF, Word2Vec) to encode text into numeric vectors. But before encoding we first need to clean the text data and this process to prepare (or clean) text data before encoding is called text preprocessing, this is the very first step to solve the NLP problems.\n",
        "\n",
        "\n",
        "Tokenization:\n",
        "Tokenization is about splitting strings of text into smaller pieces, or “tokens”. Paragraphs can be tokenized into sentences and sentences can be tokenized into words.\n",
        "Filtration:\n",
        "Similarly, if we are doing simple word counts, or trying to visualize our text with a word cloud, stopwords are some of the most frequently occurring words but don’t really tell us anything. We’re often better off tossing the stopwords out of the text.\n",
        "\n",
        "\n",
        "Certainly! Here's a shorter summary of the algorithm for text preprocessing:\n",
        "\n",
        "1. **Input Text**: Start with your raw text data.\n",
        "\n",
        "2. **Tokenization**: Split the text into words or tokens.\n",
        "\n",
        "3. **Filtration**: Clean the text by removing special characters and lowercasing.\n",
        "\n",
        "4. **Script Validation**: Ensure the text is in the correct script or language.\n",
        "\n",
        "5. **Stop Word Removal**: Eliminate common words like \"a\" and \"the.\"\n",
        "\n",
        "6. **Stemming**: Reduce words to their root form.\n",
        "\n",
        "7. **Output Preprocessed Text**: Save the cleaned text for analysis.\n",
        "\n",
        "8. **Evaluation**: Assess the impact of preprocessing on your analysis.\n",
        "\n",
        "9. **Iterate and Experiment**: Fine-tune preprocessing based on your specific task and data."
      ],
      "metadata": {
        "id": "Zpn3Q2iWnqBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaPYGsu2VKe0",
        "outputId": "de81323b-1e09-4a26-bf96-591d1a970bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Text preprocessing is an important step in natural language processing. It involves tokenization, filtration, script validation, stop word removal, and stemming.\n",
            "\n",
            "Tokenization:\n",
            "['Text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'involves', 'tokenization', ',', 'filtration', ',', 'script', 'validation', ',', 'stop', 'word', 'removal', ',', 'and', 'stemming', '.']\n",
            "\n",
            "Filtration:\n",
            "['text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '', 'it', 'involves', 'tokenization', '', 'filtration', '', 'script', 'validation', '', 'stop', 'word', 'removal', '', 'and', 'stemming', '']\n",
            "\n",
            "Script Validation:\n",
            "['text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', 'it', 'involves', 'tokenization', 'filtration', 'script', 'validation', 'stop', 'word', 'removal', 'and', 'stemming']\n",
            "\n",
            "Stop Word Removal:\n",
            "['text', 'preprocessing', 'important', 'step', 'natural', 'language', 'processing', 'involves', 'tokenization', 'filtration', 'script', 'validation', 'stop', 'word', 'removal', 'stemming']\n",
            "\n",
            "Stemming:\n",
            "['text', 'preprocess', 'import', 'step', 'natur', 'languag', 'process', 'involv', 'token', 'filtrat', 'script', 'valid', 'stop', 'word', 'remov', 'stem']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"Text preprocessing is an important step in natural language processing. It involves tokenization, filtration, script validation, stop word removal, and stemming.\"\n",
        "\n",
        "# Tokenization: Split the text into words or tokens\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Filtration: Remove non-alphanumeric characters and convert to lowercase\n",
        "filtered_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token).lower() for token in tokens]\n",
        "\n",
        "'''\n",
        "re.sub() function is used to substitute (replace) all characters in token that are not letters (a to z and A to Z) or digits (0 to 9) with an empty string ''.\n",
        "\n",
        "'''\n",
        "\n",
        "# Script Validation: You can use regular expressions to validate scripts (e.g., only keep words with Latin characters - letters from english alphabets, both uppercase and lowercase)\n",
        "latin_tokens = [token for token in filtered_tokens if re.match('^[a-zA-Z]+$', token)]\n",
        "\n",
        "# Stop Word Removal: Remove common stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens_no_stop = [token for token in latin_tokens if token not in stop_words]\n",
        "\n",
        "# Stemming: Reduce words to their root form using Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens_no_stop]\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nTokenization:\")\n",
        "print(tokens)\n",
        "print(\"\\nFiltration:\")\n",
        "print(filtered_tokens)\n",
        "print(\"\\nScript Validation:\")\n",
        "print(latin_tokens)\n",
        "print(\"\\nStop Word Removal:\")\n",
        "print(filtered_tokens_no_stop)\n",
        "print(\"\\nStemming:\")\n",
        "print(stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zYXMuWtVw7S",
        "outputId": "c519e70f-6bc4-42eb-8c11-908a359abaec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 2 - Study Morphological Analysis\n",
        "\n",
        "\n",
        "Theory:\n",
        "While performing the morphological analysis, each particular word is analyzed. Non-word tokens such as\n",
        "punctuation are removed from the words. Hence the remaining words are assigned categories. For\n",
        "instance, Ram’s iPhone cannot convert the video from .mkv to .mp4. In Morphological analysis, word by\n",
        "word the sentence is analyzed. So here, Ram is a proper noun, Ram’s is assigned as possessive suffix and\n",
        ".mkv and .mp4 is assigned as a file extension.\n",
        "\n",
        "Algo:\n",
        "\n",
        "1. **Input Word**: Start with a word you want to analyze.\n",
        "\n",
        "2. **Tokenization**: If the input text contains multiple words, split it into individual words.\n",
        "\n",
        "3. **Lemmatization**: Reduce the word to its base or dictionary form (lemma). This helps to handle inflections and variants of the word.\n",
        "\n",
        "4. **Part-of-Speech Tagging**: Assign the appropriate part-of-speech tag (e.g., noun, verb, adjective) to each word.\n",
        "\n",
        "5. **Morphological Analysis**: Analyze the word's morphology, including its gender, number, tense, case, and other linguistic features.\n",
        "\n",
        "6. **Output Results**: Store or display the results of the morphological analysis, which may include the lemma, part-of-speech, and specific morphological features.\n",
        "\n",
        "7. **Iterate and Experiment**: Depending on your linguistic analysis goals, you may need to adapt the analysis steps and rules to different languages or specific text corpora.\n",
        "\n",
        "Morphological analysis is particularly important in natural language processing and computational linguistics to understand the structure and meaning of words in a given language. The specific tools and libraries you use for these tasks will depend on the programming language and NLP frameworks you're working with."
      ],
      "metadata": {
        "id": "LvegtaVun0V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "text = \"Morphological analysis involves breaking down words into their constituent morphemes.\"\n",
        "\n",
        "# Tokenization: Split the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize a stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming: Reduce words to their root form using Porter Stemmer\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Lemmatization: Reduce words to their base or dictionary form\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nTokenization:\")\n",
        "print(tokens)\n",
        "print(\"\\nStemming:\")\n",
        "print(stemmed_tokens)\n",
        "print(\"\\nLemmatization:\")\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtoYSocW01S",
        "outputId": "94541e80-27ef-478b-a493-9bd9a38f6323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Morphological analysis involves breaking down words into their constituent morphemes.\n",
            "\n",
            "Tokenization:\n",
            "['Morphological', 'analysis', 'involves', 'breaking', 'down', 'words', 'into', 'their', 'constituent', 'morphemes', '.']\n",
            "\n",
            "Stemming:\n",
            "['morpholog', 'analysi', 'involv', 'break', 'down', 'word', 'into', 'their', 'constitu', 'morphem', '.']\n",
            "\n",
            "Lemmatization:\n",
            "['Morphological', 'analysis', 'involves', 'breaking', 'down', 'word', 'into', 'their', 'constituent', 'morpheme', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 3 - Study N-gram\n",
        "\n",
        "Theory:\n",
        "\n",
        "Given a sequence of N-1 words, an N-gram model predicts the most probable word that might follow\n",
        "this sequence. It's a probabilistic model that's trained on a corpus of text. Such a model is useful in many\n",
        "NLP applications including speech recognition, machine translation and predictive text input.\n",
        "An N-gram model is built by counting how often word sequences occur in corpus text and then\n",
        "estimating the probabilities. Since a simple N-gram model has limitations, improvements are often made\n",
        "via smoothing, interpolation and backoff. An N-gram model is one type of a Language Model (LM),\n",
        "which is about finding the probability distribution over word sequences.\n",
        "Consider two sentences: \"There was heavy rain\" vs. \"There was heavy flood\". From experience, we know\n",
        "that the former sentence sounds better. An N-gram model will tell us that \"heavy rain\" occurs much\n",
        "more often than \"heavy flood\" in the training corpus. Thus, the first sentence is more probable and will\n",
        "be selected by the model\n",
        "\n",
        "Algo"
      ],
      "metadata": {
        "id": "AQcQ9Ufwn5cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"N-grams are a sequence of items in a text or speech.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Function to generate N-grams\n",
        "def generate_ngrams(text, n):\n",
        "    n_grams = ngrams(text, n)\n",
        "    return [' '.join(gram) for gram in n_grams]\n",
        "\n",
        "# Specify the value of N for N-grams\n",
        "n = 3  # You can change this value to generate different N-grams (e.g., 2 for bigrams, 4 for 4-grams, etc.)\n",
        "\n",
        "# Generate N-grams\n",
        "ngram_list = generate_ngrams(tokens, n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"{n}-grams:\")\n",
        "for ngram in ngram_list:\n",
        "    print(ngram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG0MtYnOXMfU",
        "outputId": "7a8204a6-b815-48eb-f062-4c0a936fdf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: N-grams are a sequence of items in a text or speech.\n",
            "3-grams:\n",
            "N-grams are a\n",
            "are a sequence\n",
            "a sequence of\n",
            "sequence of items\n",
            "of items in\n",
            "items in a\n",
            "in a text\n",
            "a text or\n",
            "text or speech\n",
            "or speech .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 4 - POS Tagging\n",
        "\n",
        "Theory:It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having\n",
        "a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun,\n",
        "adjective, verb, and so on.\n",
        "Default tagging is a basic step for the part-of-speech tagging. It is performed using the DefaultTagger\n",
        "class. The DefaultTagger class takes ‘tag’ as a single argument. NN is the tag for a singular noun.\n",
        "DefaultTagger is most useful when it gets to work with the most common part-of-speech tag. That's why\n",
        "a noun tag is recommended.\n",
        "Tagging is a kind of classification that may be defined as the automatic assignment of description to the\n",
        "tokens. Here the descriptor is called tag, which may represent one part-of-speech, semantic information\n",
        "and so on.\n",
        "\n",
        "Algo:\n",
        "\n",
        "Input:\n",
        "A sentence or text document.\n",
        "\n",
        "Tokenization:\n",
        "Begin by tokenizing the input text into words or terms. This can be done using simple whitespace-based tokenization or more advanced techniques, such as regular expressions.\n",
        "\n",
        "Preprocessing:\n",
        "Remove any punctuation, special characters, or unwanted symbols from the tokens to ensure that only words or meaningful tokens are processed.\n",
        "\n",
        "Initialization:\n",
        "Initialize an empty list to store the POS tags for each word in the input text.\n",
        "\n",
        "Tag Dictionary:\n",
        "Use a pre-built dictionary or lexicon that maps words to their likely POS tags. This dictionary can be based on language-specific rules, training data, or established databases like WordNet.\n",
        "\n",
        "POS Tagging:\n",
        "Iterate through the tokens in the input text, one by one.\n",
        "For each token, consult the tag dictionary to determine its probable POS tag based on its context.\n",
        "Apply context-based rules, if available, to disambiguate POS tags when a word may have multiple possible tags.\n",
        "Add the determined POS tag to the list created in the initialization step.\n",
        "\n",
        "Output:\n",
        "After processing all tokens in the input text, you will have a list of words with their corresponding POS tags. This list represents the POS-tagged text.\n",
        "\n"
      ],
      "metadata": {
        "id": "yib9Z6DFn-Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"Part-of-speech tagging is an essential task in natural language processing.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nPOS Tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVrpqUJYeKdB",
        "outputId": "7925c45d-ffb3-4660-8810-df81f7826c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Part-of-speech tagging is an essential task in natural language processing.\n",
            "\n",
            "POS Tags:\n",
            "Part-of-speech: JJ\n",
            "tagging: NN\n",
            "is: VBZ\n",
            "an: DT\n",
            "essential: JJ\n",
            "task: NN\n",
            "in: IN\n",
            "natural: JJ\n",
            "language: NN\n",
            "processing: NN\n",
            ".: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 5 - Chunking\n",
        "\n",
        "Theory: Chunk extraction or partial parsing is a process of meaningful extracting short phrases from the sentence (tagged with Part-of-Speech). Chunks are made up of words and the kinds of words are defined using the part-of-speech tags. One can even define a pattern or words that can’t be apart of chuck and such words are known as chinks. A ChunkRule class specifies what words or patterns to include and exclude in a chunk.\n",
        "Defining Chunk patterns: Chuck patterns are normal regular expressions which are modified and designed to match the part-ofspeech tag designed to match sequences of part-of-speech tags. Angle brackets are used to specify an individual tag for example – to match a noun tag. One can define multiple tags in the same way. Chunking up or down allows the speaker to use certain language patterns, to utilize the natural internal process through language, to reach for higher meanings or search for more specific bits/portions of missing information. When we “Chunk Up” the language gets more abstract and there are more chances for agreement, and when we “Chunk Down” we tend to be looking for the specific details that may have been missing in the chunk up.\n",
        "\n",
        "Algo:\n",
        "Input:\n",
        "A sentence or text document with part-of-speech (POS)-tagged words.\n",
        "\n",
        "Tokenization and POS Tagging:\n",
        "Begin by tokenizing the input text into words or terms.\n",
        "Tag each word with its POS (Part-of-Speech) label. This can be done using an existing POS tagging tool or library.\n",
        "\n",
        "Initialization:\n",
        "Initialize an empty list to store the chunks.\n",
        "\n",
        "Define Chunking Patterns:\n",
        "Define the patterns or rules for chunking based on POS tags. These patterns can include regular expressions or syntactic rules.\n",
        "For example, a common pattern to extract noun phrases (NP) is to look for sequences of words with the following structure: (Adjective)*(Noun)+.\n",
        "\n",
        "Chunking:\n",
        "Iterate through the list of POS-tagged words.\n",
        "For each word, check if it matches any of the defined chunking patterns.\n",
        "If a pattern is matched, create a chunk that includes the words that matched the pattern.\n",
        "Continue this process for the entire sentence, extracting and storing chunks as they are encountered.\n",
        "\n",
        "Output:\n",
        "After processing the entire sentence, you will have a list of chunks, each representing a group of related words. These chunks may correspond to noun phrases, verb phrases, or other syntactic structures in the text.\n"
      ],
      "metadata": {
        "id": "CXNK22OGoDR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing is a subfield of artificial intelligence that deals with the interaction between computers and human language.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define a chunking grammar\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # Define a simple grammar for noun phrases (DT: determiner, JJ: adjective, NN: noun)\n",
        "\n",
        "# Create a chunk parser\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "\n",
        "# Parse the part-of-speech tagged text\n",
        "chunked_text = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nChunked Text:\")\n",
        "print(chunked_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsC-vlwDeVH-",
        "outputId": "e0d35a83-1212-4861-c31d-991e409821ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Natural language processing is a subfield of artificial intelligence that deals with the interaction between computers and human language.\n",
            "\n",
            "Chunked Text:\n",
            "(S\n",
            "  (NP Natural/JJ language/NN)\n",
            "  (NP processing/NN)\n",
            "  is/VBZ\n",
            "  (NP a/DT subfield/NN)\n",
            "  of/IN\n",
            "  (NP artificial/JJ intelligence/NN)\n",
            "  that/IN\n",
            "  deals/NNS\n",
            "  with/IN\n",
            "  (NP the/DT interaction/NN)\n",
            "  between/IN\n",
            "  computers/NNS\n",
            "  and/CC\n",
            "  (NP human/JJ language/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 6 - Name Entity Recognisation\n",
        "\n",
        "\n",
        "Theory:Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP) that involves identifying and classifying named entities within text. Named entities are specific entities in text, such as names of persons, organizations, locations, dates, monetary values, percentages, and more. NER is a fundamental component in various NLP applications, including information retrieval, question answering, text summarization, and language understanding.\n",
        "\n",
        "Algo:\n",
        "Input:\n",
        "A text document or sentence.\n",
        "\n",
        "Tokenization:\n",
        "Tokenize the input text into words or terms. This is typically the first step in NER.\n",
        "\n",
        "Part-of-Speech Tagging:\n",
        "Perform Part-of-Speech tagging on the tokens to determine the grammatical category of each word (e.g., noun, verb, adjective).\n",
        "\n",
        "Named Entity Recognition:\n",
        "Use NER models or libraries that have been trained on large annotated datasets to identify named entities in the text.\n",
        "These models employ techniques such as rule-based methods, machine learning, and deep learning (e.g., Conditional Random Fields, BiLSTM-CRF, or Transformers) to classify words or phrases into predefined categories, such as:\n",
        "PERSON: Names of people\n",
        "ORGANIZATION: Names of companies, institutions, or organizations\n",
        "LOCATION: Names of places, cities, countries, or regions\n",
        "DATE: Dates and temporal expressions\n",
        "MONEY: Monetary values\n",
        "PERCENT: Percentage values\n",
        "...and other custom categories depending on the application.\n",
        "\n",
        "Classification:\n",
        "Assign a category label to each identified named entity. For example, \"John Smith\" might be labeled as a PERSON, \"Apple Inc.\" as an ORGANIZATION, \"New York\" as a LOCATION, and \"July 12, 2020\" as a DATE.\n",
        "\n",
        "Output:\n",
        "Return the text with named entities highlighted or labeled according to their categories."
      ],
      "metadata": {
        "id": "V8_PkyiGoHsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the NLTK data for POS tagging\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the POS tags\n",
        "for word, pos_tag in pos_tags:\n",
        "    print(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb-OXxs8eYJX",
        "outputId": "46bce2ee-83ad-4075-94e4-12cf96a9ccbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The: DT\n",
            "quick: JJ\n",
            "brown: NN\n",
            "fox: NN\n",
            "jumps: VBZ\n",
            "over: IN\n",
            "the: DT\n",
            "lazy: JJ\n",
            "dog: NN\n",
            ".: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 7 - Create append and remove list\n",
        "\n",
        "\n",
        "Theory:\n",
        "\n",
        "In programming, you often need to work with lists, which are ordered collections of elements. Two fundamental operations are appending elements to a list and removing elements from a list.\n",
        "\n",
        "Appending to a List:\n",
        "\n",
        "Appending means adding an element to the end of a list.\n",
        "This operation can be performed in constant time because you are simply adding one element to the existing list.\n",
        "Removing from a List:\n",
        "\n",
        "Removing an element from a list can be done based on the index (position) of the element or based on the value.\n",
        "Removal by index typically involves shifting elements in the list to fill the gap left by the removed element.\n",
        "Removal by value searches for the element and then removes it.\n",
        "\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "Add element to the end of the list lst.\n",
        "The list is now updated with the new element at the end.\n",
        "Algorithm for Removing from a List by Index:\n",
        "\n",
        "Input:\n",
        "\n",
        "A list lst.\n",
        "An index index representing the position of the element to be removed.\n",
        "Algorithm:\n",
        "\n",
        "Check if index is within the valid range (0 to len(lst) - 1). If not, handle the out-of-bounds case if needed.\n",
        "If the index is valid, remove the element at that index from the list.\n",
        "Shift the elements to the right of the removed element to fill the gap.\n",
        "The list is now updated with the specified element removed.\n",
        "Algorithm for Removing from a List by Value:\n",
        "\n",
        "Input:\n",
        "\n",
        "A list lst.\n",
        "A value value to be removed.\n",
        "Algorithm:\n",
        "\n",
        "Search the list to find the first occurrence of value.\n",
        "If found, remove the element from the list.\n",
        "Shift the elements to the right of the removed element to fill the gap.\n",
        "The list is now updated with the specified value removed.\n"
      ],
      "metadata": {
        "id": "f2jm-2UToN4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list\n",
        "my_list = []\n",
        "\n",
        "# Append items to the list\n",
        "my_list.append(10)\n",
        "my_list.append(20)\n",
        "my_list.append(30)\n",
        "\n",
        "# Display the list\n",
        "print(\"Initial List:\", my_list)\n",
        "\n",
        "# Remove an item by value\n",
        "value_to_remove = 20\n",
        "if value_to_remove in my_list:\n",
        "    my_list.remove(value_to_remove)\n",
        "\n",
        "# Display the list after removal\n",
        "print(\"List after removing\", value_to_remove, \":\", my_list)\n",
        "\n",
        "# Remove an item by index\n",
        "index_to_remove = 1\n",
        "if 0 <= index_to_remove < len(my_list):\n",
        "    removed_item = my_list.pop(index_to_remove)\n",
        "    print(\"Removed item at index\", index_to_remove, \":\", removed_item)\n",
        "\n",
        "# Display the list after removal by index\n",
        "print(\"List after removal by index:\", my_list)\n",
        "\n",
        "\n",
        "\n",
        "# ------ method 2\n",
        "myList = [10,20,30,40,50]\n",
        "\n",
        "value_to_remove = 40\n",
        "for i in range(len(myList)):\n",
        "    if value_to_remove == myList[i]:\n",
        "        myList.remove(value_to_remove)\n",
        "        print(f'{value_to_remove} removed! New list- \\n')\n",
        "        print(myList)\n",
        "        break\n",
        "\n",
        "index_to_remove = 2\n",
        "if i < len(myList):\n",
        "    myList.pop(i)\n",
        "\n",
        "print(myList)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUTxO3WWemae",
        "outputId": "9f8ecfc9-4538-4713-e970-ddf3002afba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial List: [10, 20, 30]\n",
            "List after removing 20 : [10, 30]\n",
            "Removed item at index 1 : 30\n",
            "List after removal by index: [10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 8 - Parsing and Context free Grammar\n",
        "\n",
        "Theory:\n",
        "\n",
        "Context-Free Grammar (CFG) Theory:\n",
        "Context-Free Grammar is a formalism used in linguistics and computer science to describe the syntax of a language. It consists of a set of production rules that define how sentences or phrases can be constructed in the language.\n",
        "\n",
        "Parsing:\n",
        "Parsing is the process of analyzing a sequence of symbols (usually text or code) to determine its grammatical structure with respect to a given formal grammar, such as a CFG. Parsing is a crucial step in language understanding, as it enables computers to interpret and process human language or programming code.\n",
        "\n",
        "Algo:\n",
        "\n",
        "Initialization:\n",
        "Create an empty stack for parsing.\n",
        "Push the start symbol S onto the stack.\n",
        "Initialize a pointer at the beginning of the input string.\n",
        "\n",
        "Parsing Loop:\n",
        "Repeat the following until the stack is empty or the input is fully consumed:\n",
        "If the top of the stack is a non-terminal symbol A:\n",
        "Consult the CFG rules to find a production rule A → β that matches the current input symbol.\n",
        "Push the symbols in β onto the stack, replacing A.\n",
        "If the top of the stack is a terminal symbol that matches the current input symbol:\n",
        "Pop the stack and advance the input pointer.\n",
        "\n",
        "Completion Check:\n",
        "If the stack is empty and the input is fully consumed, parsing is successful.\n",
        "If the stack still contains symbols or the input is not fully consumed, parsing fails.\n",
        "\n",
        "Output:\n",
        "If parsing is successful, you can construct a parse tree or extract information about the structure of the input. If parsing fails, report an error or provide diagnostic information."
      ],
      "metadata": {
        "id": "WpPLB11Q5Bzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Define a context-free grammar\n",
        "cfg = nltk.CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N | 'I'\n",
        "    VP -> V NP\n",
        "    Det -> 'the' | 'an'\n",
        "    N -> 'cat' | 'dog'\n",
        "    V -> 'chased' | 'saw'\n",
        "\"\"\")\n",
        "\n",
        "# Create a parser with the defined CFG\n",
        "parser = nltk.ChartParser(cfg)\n",
        "\n",
        "# Define a sentence to parse\n",
        "sentence = \"I saw the cat\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Parse the sentence\n",
        "for tree in parser.parse(tokens):\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1NZoRV2erxp",
        "outputId": "7ea8b990-eccf-43f2-aab3-990bed01e1ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         S             \n",
            "  _______|___           \n",
            " |           VP        \n",
            " |    _______|___       \n",
            " |   |           NP    \n",
            " |   |        ___|___   \n",
            " NP  V      Det      N \n",
            " |   |       |       |  \n",
            " I  saw     the     cat\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp 9 - Implementation of Named Entity Recognition\n",
        "\n",
        "Theory:\n",
        "\n",
        "Named Entity Recognition (NER) Theory:\n",
        "\n",
        "Named Entity Recognition (NER) is a subtask of information extraction that involves identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations, dates, monetary values, percentages, and more. NER plays a crucial role in various natural language processing applications, including information retrieval, question answering, and language understanding.\n",
        "\n",
        "Algo:\n",
        "\n",
        "Data Preparation:\n",
        "Collect and preprocess your training data, which includes labeled examples of text with named entities tagged (e.g., with entity types like PERSON, ORGANIZATION, LOCATION).\n",
        "\n",
        "Feature Extraction:\n",
        "Convert the text data into numerical features that can be used by a machine learning model. Common features include word embeddings, character-level representations, or subword embeddings (e.g., Word2Vec, GloVe, or BERT embeddings).\n",
        "\n",
        "Model Selection:\n",
        "Choose a machine learning model for sequence labeling. Common choices include:\n",
        "Conditional Random Fields (CRF)\n",
        "Bi-directional LSTM (Long Short-Term Memory) with CRF\n",
        "Pre-trained Transformer-based models like BERT or GPT-3 fine-tuned for NER.\n",
        "\n",
        "Training:\n",
        "Train the selected model using the preprocessed and labeled training data. The model learns to predict named entities within the text.\n",
        "\n",
        "Testing:\n",
        "Apply the trained model to new, unlabeled text data to identify named entities. The model assigns entity labels to spans of text.\n",
        "\n",
        "Post-processing:\n",
        "Depending on the model output, you may need to post-process the results to group consecutive tokens into entity spans and assign entity types.\n",
        "\n",
        "Output:\n",
        "The output will be the text with identified named entities and their associated categories."
      ],
      "metadata": {
        "id": "ecary2oK5HfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_word(length=8):\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Generate a random word of 8 characters\n",
        "random_word = generate_random_word()\n",
        "print(random_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH98pTGVexga",
        "outputId": "cfd902ee-bbe0-4141-d316-3f864ca07751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tybqepor\n"
          ]
        }
      ]
    }
  ]
}