{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab8d5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\choud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\choud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Morphological analysis involves breaking down words into their constituent morphemes.\n",
      "\n",
      "Tokenization:\n",
      "['Morphological', 'analysis', 'involves', 'breaking', 'down', 'words', 'into', 'their', 'constituent', 'morphemes', '.']\n",
      "\n",
      "Stemming:\n",
      "['morpholog', 'analysi', 'involv', 'break', 'down', 'word', 'into', 'their', 'constitu', 'morphem', '.']\n",
      "\n",
      "Lemmatization:\n",
      "['Morphological', 'analysis', 'involves', 'breaking', 'down', 'word', 'into', 'their', 'constituent', 'morpheme', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"Morphological analysis involves breaking down words into their constituent morphemes.\"\n",
    "\n",
    "# Tokenization: Split the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize a stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stemming: Reduce words to their root form using Porter Stemmer\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatization: Reduce words to their base or dictionary form\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokenization:\")\n",
    "print(tokens)\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nidhi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens - \n",
      "['This', 'is', 'assumed', 'to', 'be', 'a', 'list', 'of', 'tokens', 'that', 'have', 'been', 'previously', 'processed', 'and', 'cleaned', '', 'likely', 'by', 'removing', 'nonalphanumeric', 'characters', 'and', 'converting', 'the', 'tokens', 'to', 'lowercase', '']\n",
      "\n",
      "Stop words removal - \n",
      "['This', 'assumed', 'list', 'tokens', 'previously', 'processed', 'cleaned', '', 'likely', 'removing', 'nonalphanumeric', 'characters', 'converting', 'tokens', 'lowercase', '']\n",
      "\n",
      "Stemmed tokens - \n",
      "['thi', 'assum', 'list', 'token', 'previous', 'process', 'clean', '', 'like', 'remov', 'nonalphanumer', 'charact', 'convert', 'token', 'lowercas', '']\n",
      "\n",
      "Lemmatized tokens - \n",
      "['This', 'assumed', 'list', 'token', 'previously', 'processed', 'cleaned', '', 'likely', 'removing', 'nonalphanumeric', 'character', 'converting', 'token', 'lowercase', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "text = 'This is assumed to be a list of tokens that have been previously processed and cleaned, likely by removing non-alphanumeric characters and converting the tokens to lowercase.'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered_tokens_no_stop_words = [token for token in filtered_tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens_no_stop_words]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens_no_stop_words]\n",
    "\n",
    "print(f'Filtered tokens - \\n{filtered_tokens}\\n')\n",
    "print(f'Stop words removal - \\n{filtered_tokens_no_stop_words}\\n')\n",
    "print(f'Stemmed tokens - \\n{stemmed_tokens}\\n')\n",
    "print(f'Lemmatized tokens - \\n{lemmatized_tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are both text normalization techniques used in natural language processing (NLP) and information retrieval to reduce words to their base or root forms. While they serve the same fundamental purpose, they achieve this goal using different approaches and have distinct characteristics:\n",
    "\n",
    "1. Stemming:\n",
    "\n",
    "Approach: Stemming algorithms reduce words to their stem by removing common prefixes or suffixes. Stemmers use heuristic rules to cut off prefixes or suffixes to obtain the base form of a word.\n",
    "Result: The resulting stems may not be valid words; they are simply substrings that have linguistic meaning. For example, \"running\" and \"ran\" would both be stemmed to \"run.\"\n",
    "Speed: Stemming is generally faster because it uses simple rules and string manipulation techniques.\n",
    "Example: For the word \"running,\" a stemming algorithm might remove the \"-ing\" suffix, resulting in the stem \"run.\"\n",
    "\n",
    "2. Lemmatization:\n",
    "\n",
    "Approach: Lemmatization, on the other hand, reduces words to their lemma or dictionary form. Lemmatizers use vocabulary and morphological analysis to find the lemma that represents the word's base form.\n",
    "Result: The resulting lemmas are actual words found in the dictionary and have valid linguistic meanings. Lemmatization ensures that words are reduced to their proper dictionary form.\n",
    "Accuracy: Lemmatization is more accurate than stemming because it takes into account the word's meaning and context.\n",
    "Example: For the word \"running,\" a lemmatization algorithm would map it to the lemma \"run,\" considering its correct grammatical and semantic form.\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "Stemming is faster but may not always produce valid words, whereas lemmatization is slower but ensures valid words are obtained.\n",
    "Lemmatization considers the context and meaning of words, making it more accurate than stemming.\n",
    "Stemming is useful when speed is a priority, and slight inaccuracies or non-words are acceptable. Lemmatization is preferable when accuracy and meaningful results are crucial, such as in language understanding applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32828e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
